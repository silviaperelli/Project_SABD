services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    hostname: namenode
    ports:
      - "9870:9870" # Namenode Web UI
      - "8020:8020" # Namenode RPC
    volumes:
      - namenode_data:/hadoop/dfs/name
      - ./hadoop_init_scripts:/opt/hadoop_init_scripts
      - ./namenode-entrypoint.sh:/custom-entrypoint.sh
      - ./hadoop_conf:/opt/hadoop/etc/hadoop
    entrypoint: /custom-entrypoint.sh
    environment:
      - CLUSTER_NAME=test
    networks:
      - sabd_net

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    hostname: datanode
    volumes:
      - datanode_data:/hadoop/dfs/data
      - ./hadoop_conf:/opt/hadoop/etc/hadoop
    environment:
      - SERVICE_PRECONDITION=namenode:9870 # Wait for namenode to be up
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    depends_on:
      - namenode
    networks:
      - sabd_net

  nifi:
    image: apache/nifi:1.25.0 # Or latest, but pinning version is good practice
    container_name: nifi
    restart: always
    ports:
      - "8443:8443" # NiFi Web UI (HTTPS by default in recent versions)
    volumes:
      - ./nifi_conf/templates:/opt/nifi/nifi-current/conf/templates
      - ./hadoop_conf:/opt/nifi/hadoop_conf # Mount HDFS configs
      - nifi_data_conf:/opt/nifi/nifi-current/conf
      - nifi_data_database:/opt/nifi/nifi-current/database_repository
      - nifi_data_flowfile:/opt/nifi/nifi-current/flowfile_repository
      - nifi_data_content:/opt/nifi/nifi-current/content_repository
      - nifi_data_provenance:/opt/nifi/nifi-current/provenance_repository
      - nifi_data_state:/opt/nifi/nifi-current/state
      - nifi_data_logs:/opt/nifi/nifi-current/logs
    environment:
      - NIFI_WEB_HTTPS_PORT=8443
      - NIFI_SENSITIVE_PROPS_KEY= VQHGn0eLMqfDBaElpeZGh3atsh9TwI4s
    depends_on:
      - namenode
      - datanode
    networks:
      - sabd_net
    healthcheck:
      test: ["CMD", "curl", "-f", "-k", "https://localhost:8443/nifi-api/system-diagnostics"] # -k per insecure HTTPS
      interval: 30s
      timeout: 10s
      retries: 5

  spark-master:
    image: bitnami/spark:3.5 # Spark 3.5 Ã¨ compatibile con Hadoop 3.2.1
    container_name: spark-master
    hostname: spark-master
    ports:
      - "8081:8080"  # Spark Master Web UI (8081 host per evitare conflitti)
      - "7077:7077"  # Spark Master RPC
    volumes:
      - ./spark_apps:/opt/spark_apps # Monta i tuoi script PySpark qui
      - ./hadoop_conf:/opt/bitnami/spark/conf/hadoop:ro # Monta la config di Hadoop (sola lettura)
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - HADOOP_CONF_DIR=/opt/bitnami/spark/conf/hadoop
      - LD_PRELOAD=/opt/bitnami/common/lib/libnss_wrapper.so # <--- AGGIUNGI QUI
      - HADOOP_USER_NAME=spark
      - USER=spark
      - LOGNAME=spark
      - HOME=/opt/bitnami/spark
    networks:
      - sabd_net
    depends_on:
      namenode:
        condition: service_healthy # Spark ha bisogno che HDFS Namenode sia pronto
      datanode: # I worker e il driver Spark avranno bisogno dei datanode
        condition: service_started # Datanode non ha un healthcheck di default
    healthcheck: # Healthcheck per il master Spark
      test: ["CMD", "curl", "-f", "http://localhost:8080/api/v1/applications"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  spark-worker:
    image: bitnami/spark:3.5
    container_name: spark-worker-1 # Puoi scalare aggiungendo spark-worker-2, ecc.
    hostname: spark-worker-1
    volumes:
      - ./hadoop_conf:/opt/bitnami/spark/conf/hadoop:ro # Anche i worker necessitano della config HDFS
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=1 # Configura le risorse a seconda della tua macchina
      - SPARK_WORKER_MEMORY=1G # Configura le risorse
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - HADOOP_CONF_DIR=/opt/bitnami/spark/conf/hadoop
      - SPARK_WORKER_WEBUI_PORT=8082 # Porta UI per questo worker specifico (opzionale, per debug)
    depends_on:
      spark-master:
        condition: service_healthy
      namenode: # I worker potrebbero aver bisogno di accedere a HDFS per log o altro
        condition: service_healthy
    networks:
      - sabd_net

volumes:
  namenode_data:
  datanode_data:
  nifi_data_conf:
  nifi_data_database:
  nifi_data_flowfile:
  nifi_data_content:
  nifi_data_provenance:
  nifi_data_state:
  nifi_data_logs:

networks:
  sabd_net:
    driver: bridge